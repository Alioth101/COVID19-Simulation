"""
Multi-Key OpenAI Backend - Load balancing across multiple API keys
æ”¯æŒå¤šä¸ªAPIå¯†é’¥è½®è¯¢ä»¥çªç ´TPMé™åˆ¶
"""

import os
import time
import traceback
import itertools
from typing import List, Dict
from threading import Lock
from datetime import datetime, timedelta

from .openai_backend import OpenAIBackend


class MultiKeyOpenAIBackend(OpenAIBackend):
    """
    Multi-key OpenAI backend with automatic load balancing
    
    Features:
    - Round-robin distribution across multiple API keys
    - Automatic failover to next key on error
    - Thread-safe key rotation
    - 2x-3x throughput improvement with 2-3 keys
    - ğŸ›¡ï¸ Global cooldown protection (4th layer of protection)
    
    Protection Layers:
    1. Retry mechanism (max_retries)
    2. Exponential backoff
    3. Multi-key rotation
    4. ğŸ†• Global cooldown (10 min pause on critical errors)
    
    Usage:
        # Method 1: Environment variables
        export OPENAI_API_KEY_1="sk-xxx-key1"
        export OPENAI_API_KEY_2="sk-xxx-key2"
        
        backend = MultiKeyOpenAIBackend(
            api_keys=None,  # Auto-detect from env
            model_name="deepseek-ai/DeepSeek-V3.2-Exp"
        )
        
        # Method 2: Direct list
        backend = MultiKeyOpenAIBackend(
            api_keys=["sk-xxx-key1", "sk-xxx-key2"],
            model_name="deepseek-ai/DeepSeek-V3.2-Exp"
        )
    """
    
    # ============================================================================
    # ğŸ›¡ï¸ ç¬¬å››é‡ä¿æŠ¤æœºåˆ¶ï¼šå…¨å±€å†·å´çŠ¶æ€ï¼ˆç±»çº§åˆ«å˜é‡ï¼Œæ‰€æœ‰å®ä¾‹å…±äº«ï¼‰
    # ============================================================================
    _global_cooldown_until = None  # å†·å´ç»“æŸæ—¶é—´
    _cooldown_lock = Lock()        # çº¿ç¨‹å®‰å…¨é”
    _cooldown_duration = 600       # å†·å´æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10åˆ†é’Ÿ
    
    def __init__(
        self,
        api_keys: List[str] = None,
        model_name: str = "gpt-4o-mini",
        temperature: float = 0.7,
        max_tokens: int = 2000,
        base_url: str = None,
        proxies: List = None
    ):
        """
        Initialize multi-key backend
        
        Args:
            api_keys: List of API keys. If None, auto-detect from environment:
                - OPENAI_API_KEY_1, OPENAI_API_KEY_2, ..., OPENAI_API_KEY_N
                - Or OPENAI_API_KEY (fallback to single key)
            model_name: Model to use
            temperature: Sampling temperature
            max_tokens: Max response tokens
            base_url: Custom API base URL (applies to all keys)
            proxies: List of proxy configurations for each API key.
                Format: ["http://user:pass@host:port", ...] or
                        [{"http": "...", "https": "..."}, ...]
                If None, no proxies are used.
                Must match the length of api_keys if provided.
        """
        # Auto-detect API keys from environment
        if api_keys is None:
            api_keys = self._auto_detect_keys()
        
        if not api_keys:
            raise ValueError(
                "No API keys provided. Please either:\n"
                "  1. Set environment variables:\n"
                "     - OPENAI_API_KEY_1='sk-xxx-key1'\n"
                "     - OPENAI_API_KEY_2='sk-xxx-key2'\n"
                "     - ... (add more as needed)\n"
                "  2. Pass api_keys=[key1, key2, ...] parameter"
            )
        
        self.api_keys = api_keys
        self.num_keys = len(api_keys)
        self.base_url = base_url or os.getenv("OPENAI_BASE_URL")
        
        # Proxy configuration
        self.proxies = proxies or [None] * self.num_keys
        if len(self.proxies) != self.num_keys:
            raise ValueError(
                f"Proxies list length ({len(self.proxies)}) must match "
                f"api_keys list length ({self.num_keys})"
            )
        
        # ğŸ”§ [ENHANCED] ä¸ºæ¯ä¸ªAPI keyåˆ›å»ºæ˜“äºè¯†åˆ«çš„æ ‡ç­¾
        self.key_labels = []
        for i, key in enumerate(api_keys):
            label = f"Key#{i+1}[{key[:10]}...{key[-4:]}]"
            self.key_labels.append(label)
        
        # Initialize with first key
        super().__init__(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            api_key=api_keys[0],
            base_url=self.base_url
        )
        
        # Create clients for all keys
        self.clients = []
        for i, key in enumerate(api_keys):
            proxy = self.proxies[i]
            proxy_info = ""
            if proxy:
                # éšè—ä»£ç†æ•æ„Ÿä¿¡æ¯ç”¨äºæ—¥å¿—æ˜¾ç¤º
                if isinstance(proxy, dict):
                    proxy_str = proxy.get('http') or proxy.get('https') or str(proxy)
                else:
                    proxy_str = str(proxy)
                if '@' in proxy_str:
                    proxy_display = proxy_str.split('@')[0].split('//')[0] + '//' + '***@' + proxy_str.split('@')[1]
                else:
                    proxy_display = proxy_str
                proxy_info = f" with proxy {proxy_display}"
            
            print(f"[Multi-Key Backend] Initializing {self.key_labels[i]}{proxy_info}")
            client = self._create_client_for_key(key, proxy)
            self.clients.append(client)
        
        # ğŸ”§ [ENHANCED] å¯†é’¥ä½¿ç”¨ç»Ÿè®¡
        self.key_stats = [{'success': 0, 'failed': 0, 'last_error': None} for _ in api_keys]
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ğŸ†• æ™ºèƒ½å¯†é’¥è½®è½¬æœºåˆ¶ (æ¯Næ¬¡è¯·æ±‚è‡ªåŠ¨åˆ‡æ¢)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # è®¾è®¡åŸåˆ™:
        #   1. å¼€å§‹æ—¶åªä½¿ç”¨ç¬¬1ä¸ªå¯†é’¥ (current_key_index = 0)
        #   2. è·Ÿè¸ªå½“å‰å¯†é’¥çš„è¯·æ±‚æ¬¡æ•° (current_key_request_count)
        #   3. å½“è¯·æ±‚æ¬¡æ•°è¾¾åˆ°é˜ˆå€¼(rotation_interval)æ—¶,è‡ªåŠ¨åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥
        #   4. æ— å¯ç”¨å¯†é’¥æ—¶å¾ªç¯å›ç¬¬1ä¸ª (å¾ªç¯ä½¿ç”¨)
        # 
        # ä¼˜åŠ¿:
        #   - ä¸»åŠ¨è§„é¿TPM/RPMé™åˆ¶
        #   - è´Ÿè½½å‡è¡¡æ›´å‡åŒ€
        #   - é¿å…ç­‰åˆ°æŠ¥é”™äº†æ‰åˆ‡æ¢
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        self._current_key_index = 0  # å½“å‰æ´»è·ƒçš„å¯†é’¥ç´¢å¼• (ä»ç¬¬1ä¸ªå¼€å§‹)
        self._current_key_request_count = 0  # å½“å‰å¯†é’¥å·²è¯·æ±‚æ¬¡æ•°
        
        # è·å–è½®è½¬é—´éš”é…ç½®ï¼Œé»˜è®¤ä¸º50æ¬¡ï¼ˆå¦‚æœç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼‰
        # å¦‚æœåªæœ‰ä¸€ä¸ªå¯†é’¥ï¼Œåˆ™ç¦ç”¨è½®è½¬ï¼ˆintervalè®¾ä¸ºéå¸¸å¤§çš„æ•°ï¼‰
        if self.num_keys > 1:
            self._rotation_interval = int(os.getenv("LLM_KEY_ROTATION_INTERVAL", "50"))
            print(f"[Multi-Key Backend] ğŸ†• Automatic Key Rotation Enabled:")
            print(f"                     - Strategy: Rotate every {self._rotation_interval} requests")
        else:
            self._rotation_interval = float('inf')
            print(f"[Multi-Key Backend] â„¹ï¸ Automatic Key Rotation Disabled (Single Key Mode)")
            
        self._consecutive_rate_limit_errors = [0] * self.num_keys  # æ¯ä¸ªå¯†é’¥çš„è¿ç»­é™æµé”™è¯¯æ¬¡æ•°
        self._rate_limit_threshold = 1  # ç«‹å³åˆ‡æ¢é˜ˆå€¼ (é‡åˆ°é™æµç«‹å³åˆ‡æ¢)
        self._lock = Lock()  # çº¿ç¨‹å®‰å…¨é”
        
        print(f"[Multi-Key Backend] âœ“ Initialized {self.num_keys} API keys")
        print(f"                     - Starting with: {self.key_labels[0]}")
        print(f"                     - Error Switch threshold: Immediate switch on rate limit")
        print(f"                     - Auto fallback to Key#1 when all keys exhausted")
        print(f"[Multi-Key Backend] Expected throughput boost: {self.num_keys}x")
    
    def _auto_detect_keys(self) -> List[str]:
        """Auto-detect API keys from environment variables"""
        keys = []
        
        # Try numbered keys: OPENAI_API_KEY_1, OPENAI_API_KEY_2, ...
        i = 1
        while True:
            key = os.getenv(f"OPENAI_API_KEY_{i}")
            if not key:
                break
            keys.append(key)
            i += 1
        
        # Fallback to single key
        if not keys:
            single_key = os.getenv("OPENAI_API_KEY")
            if single_key:
                keys = [single_key]
                print("[Multi-Key Backend] Warning: Only one API key found. "
                      "Add OPENAI_API_KEY_2, OPENAI_API_KEY_3, etc. for load balancing.")
        
        return keys
    
    def _create_client_for_key(self, api_key: str, proxy=None):
        """
        Create OpenAI client for a specific API key with optional proxy
        
        Args:
            api_key: API key
            proxy: Proxy configuration (string or dict)
                - String: "http://user:pass@host:port"
                - Dict: {"http": "...", "https": "..."}
                - None: No proxy
        """
        try:
            from openai import OpenAI
            import httpx
        except ImportError:
            raise ImportError(
                "openai and httpx packages not installed. Run: pip install openai>=1.0.0 httpx"
            )
        
        # Create HTTP client with proxy if configured
        http_client = None
        if proxy:
            # Convert string proxy to dict format if needed
            if isinstance(proxy, str):
                proxy_dict = {
                    "http://": proxy,
                    "https://": proxy
                }
            else:
                proxy_dict = proxy
            
            # Create httpx.Client with proxy configuration
            http_client = httpx.Client(
                proxies=proxy_dict,
                timeout=120.0  # 120 second timeout
            )
        
        # Create OpenAI client
        if self.base_url:
            return OpenAI(
                api_key=api_key, 
                base_url=self.base_url,
                http_client=http_client
            )
        else:
            return OpenAI(
                api_key=api_key,
                http_client=http_client
            )
    
    def _check_and_rotate_key(self):
        """
        æ£€æŸ¥å¹¶æ‰§è¡ŒåŸºäºè¯·æ±‚æ¬¡æ•°çš„è‡ªåŠ¨è½®è½¬
        """
        if self.num_keys <= 1:
            return

        with self._lock:
            # å†æ¬¡æ£€æŸ¥ä»¥é˜²race condition
            if self._current_key_request_count >= self._rotation_interval:
                old_index = self._current_key_index
                
                # æ‰§è¡Œåˆ‡æ¢
                self._current_key_index = (old_index + 1) % self.num_keys
                self._current_key_request_count = 0
                
                print(f"\nğŸ”„ [AUTO ROTATION] å·²è¾¾åˆ° {self._rotation_interval} æ¬¡è¯·æ±‚ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° {self.key_labels[self._current_key_index]}")
            
            # å¢åŠ è®¡æ•°
            self._current_key_request_count += 1

    def _get_current_client(self):
        """
        è·å–å½“å‰æ´»è·ƒçš„å¯†é’¥å®¢æˆ·ç«¯ (æ™ºèƒ½è½®è½¬)
        
        æ¯æ¬¡è·å–å®¢æˆ·ç«¯æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦åŸºäºè¯·æ±‚æ¬¡æ•°è¿›è¡Œè½®è½¬
        
        Returns:
            tuple: (client, key_index)
        """
        # 1. å…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦åŸºäºæ¬¡æ•°è½®è½¬
        self._check_and_rotate_key()
        
        # 2. è·å–å½“å‰å®¢æˆ·ç«¯
        with self._lock:
            key_index = self._current_key_index
        return self.clients[key_index], key_index
    
    def _should_switch_key(self, error: Exception, current_key_index: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ‡æ¢å¯†é’¥
        
        åˆ‡æ¢æ¡ä»¶:
        - é”™è¯¯æ˜¯é™æµç›¸å…³ (429, TPM, RPM, quotaç­‰)
        - å½“å‰å¯†é’¥å·²è¿ç»­å¤±è´¥3æ¬¡
        
        Args:
            error: æ•è·çš„å¼‚å¸¸
            current_key_index: å½“å‰å¯†é’¥ç´¢å¼•
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥åˆ‡æ¢å¯†é’¥
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯é™æµé”™è¯¯
        if not self._is_rate_limit_error(error):
            return False
        
        # å¢åŠ è¿ç»­é”™è¯¯è®¡æ•°
        with self._lock:
            self._consecutive_rate_limit_errors[current_key_index] += 1
            consecutive_errors = self._consecutive_rate_limit_errors[current_key_index]
        
        # è¾¾åˆ°é˜ˆå€¼æ—¶éœ€è¦åˆ‡æ¢
        return consecutive_errors >= self._rate_limit_threshold
    
    def _is_rate_limit_error(self, error: Exception) -> bool:
        """
        åˆ¤æ–­é”™è¯¯æ˜¯å¦æ˜¯é™æµç›¸å…³
        
        é™æµé”™è¯¯åŒ…æ‹¬:
        - 429 Too Many Requests
        - TPM (Tokens Per Minute) é™åˆ¶
        - RPM (Requests Per Minute) é™åˆ¶
        - Quota/é…é¢è¶…é™
        - Service overloaded
        
        Args:
            error: æ•è·çš„å¼‚å¸¸
            
        Returns:
            bool: æ˜¯å¦æ˜¯é™æµé”™è¯¯
        """
        error_msg = str(error).lower()
        
        rate_limit_patterns = [
            '429',
            'too many requests',
            'rate limit',
            'rate_limit',
            'ratelimit',
            'tpm',
            'rpm',
            'quota',
            'insufficient_quota',
            'overloaded',
            'service overloaded',
            '529',
        ]
        
        return any(pattern in error_msg for pattern in rate_limit_patterns)
    
    def _switch_to_next_key(self, failed_key_index: int, reason: str = ""):
        """
        åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨å¯†é’¥
        
        Args:
            failed_key_index: è§¦å‘åˆ‡æ¢çš„å¤±è´¥å¯†é’¥ç´¢å¼•
            reason: åˆ‡æ¢åŸå›  (ç”¨äºæ—¥å¿—)
        """
        with self._lock:
            # ğŸ›¡ï¸ é˜²æ­¢å¤šçº¿ç¨‹å¹¶å‘å¯¼è‡´çš„é‡å¤è½®è½¬
            # å¦‚æœå½“å‰å¯†é’¥å·²ç»ä¸æ˜¯é‚£ä¸ªå¤±è´¥çš„å¯†é’¥ï¼Œè¯´æ˜å·²ç»è¢«å…¶ä»–çº¿ç¨‹è½®è½¬è¿‡äº†
            if self._current_key_index != failed_key_index:
                # print(f"[Key Rotation] Skipped: Already rotated from {self.key_labels[failed_key_index]} to {self.key_labels[self._current_key_index]}")
                return

            old_index = self._current_key_index
            
            # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥ (å¾ªç¯)
            self._current_key_index = (old_index + 1) % self.num_keys
            new_index = self._current_key_index
            
            # é‡ç½®æ—§å¯†é’¥çš„è¿ç»­é”™è¯¯è®¡æ•° (å…è®¸åç»­æ¢å¤ä½¿ç”¨)
            old_consecutive_errors = self._consecutive_rate_limit_errors[old_index]
            self._consecutive_rate_limit_errors[old_index] = 0
            
            # é‡ç½®è¯·æ±‚è®¡æ•°
            self._current_key_request_count = 0
        
        # æ‰“å°åˆ‡æ¢æ—¥å¿—
        print("\n" + "="*80)
        print("ğŸ”„ [KEY ROTATION] æ™ºèƒ½å¯†é’¥è½®è½¬è§¦å‘")
        print("="*80)
        print(f"åŸå› : {reason}")
        print(f"æ—§å¯†é’¥: {self.key_labels[old_index]} (è¿ç»­å¤±è´¥{old_consecutive_errors}æ¬¡)")
        print(f"æ–°å¯†é’¥: {self.key_labels[new_index]}")
        
        if new_index == 0 and old_index == self.num_keys - 1:
            print("âš ï¸  å·²è½®è½¬å›ç¬¬1ä¸ªå¯†é’¥ (æ‰€æœ‰å¯†é’¥å·²å°è¯•ä¸€è½®)")
        
        print(f"\nğŸ“Š å½“å‰å¯†é’¥çŠ¶æ€:")
        for i in range(self.num_keys):
            consecutive = self._consecutive_rate_limit_errors[i]
            status = "ğŸŸ¢ æ´»è·ƒ" if i == new_index else f"âšª å¾…å‘½ (è¿ç»­é”™è¯¯: {consecutive}æ¬¡)"
            print(f"  {self.key_labels[i]}: {status}")
        
        print("="*80 + "\n")
    
    def _reset_consecutive_errors_on_success(self, key_index: int):
        """
        æˆåŠŸè°ƒç”¨åé‡ç½®è¯¥å¯†é’¥çš„è¿ç»­é”™è¯¯è®¡æ•°
        
        è¿™æ ·å¯ä»¥ç¡®ä¿åªæœ‰"è¿ç»­"å¤±è´¥æ‰ä¼šè§¦å‘åˆ‡æ¢,
        å¶å°”çš„å¤±è´¥ä¸ä¼šå¯¼è‡´åˆ‡æ¢ã€‚
        
        Args:
            key_index: å¯†é’¥ç´¢å¼•
        """
        with self._lock:
            if self._consecutive_rate_limit_errors[key_index] > 0:
                self._consecutive_rate_limit_errors[key_index] = 0
    
    # ============================================================================
    # ğŸ›¡ï¸ ç¬¬å››é‡ä¿æŠ¤æœºåˆ¶ï¼šå…¨å±€å†·å´ç›¸å…³æ–¹æ³•
    # ============================================================================
    
    @classmethod
    def _should_trigger_cooldown(cls, error: Exception) -> bool:
        """
        åˆ¤æ–­é”™è¯¯æ˜¯å¦åº”è¯¥è§¦å‘å…¨å±€å†·å´
        
        è§¦å‘æ¡ä»¶ï¼ˆä¸¥é‡çš„APIé™åˆ¶é”™è¯¯ï¼‰ï¼š
        - 429 Too Many Requests (rate limit)
        - 529 Service overloaded
        - å¤šæ¬¡è¿ç»­çš„503/502é”™è¯¯
        
        Args:
            error: æ•è·çš„å¼‚å¸¸
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥è§¦å‘å†·å´
        """
        error_msg = str(error).lower()
        error_type = type(error).__name__
        
        # æ£€æŸ¥HTTPçŠ¶æ€ç ç›¸å…³é”™è¯¯
        critical_patterns = [
            '429',  # Rate limit
            'too many requests',
            'rate limit',
            'quota',
            'insufficient_quota',
            '529',  # Service overloaded
            'service overloaded',
            'overloaded',
        ]
        
        for pattern in critical_patterns:
            if pattern in error_msg:
                return True
        
        return False
    
    @classmethod
    def _trigger_global_cooldown(cls, reason: str = "Critical API error"):
        """
        è§¦å‘å…¨å±€å†·å´
        
        Args:
            reason: è§¦å‘åŸå› ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        """
        with cls._cooldown_lock:
            cls._global_cooldown_until = datetime.now() + timedelta(seconds=cls._cooldown_duration)
            
            print("\n" + "="*80)
            print("ğŸ›¡ï¸  [GLOBAL COOLDOWN ACTIVATED] ç¬¬å››é‡ä¿æŠ¤æœºåˆ¶å·²è§¦å‘")
            print("="*80)
            print(f"åŸå› : {reason}")
            print(f"å†·å´æ—¶é•¿: {cls._cooldown_duration}ç§’ ({cls._cooldown_duration//60}åˆ†é’Ÿ)")
            print(f"æ¢å¤æ—¶é—´: {cls._global_cooldown_until.strftime('%H:%M:%S')}")
            print()
            print("ğŸ“Š ä¿æŠ¤æœºåˆ¶è¯´æ˜:")
            print("  â€¢ æ‰€æœ‰APIè°ƒç”¨å°†æš‚åœï¼Œé¿å…è§¦å‘æ›´ä¸¥é‡çš„é™åˆ¶")
            print("  â€¢ å®éªŒä¸ä¼šä¸­æ­¢ï¼Œä¼šåœ¨å†·å´åè‡ªåŠ¨æ¢å¤")
            print("  â€¢ è¿™å¯ä»¥ä¿æŠ¤ä½ çš„APIè´¦æˆ·ä¸è¢«å°ç¦")
            print()
            print("â±ï¸  å€’è®¡æ—¶å¼€å§‹...")
            print("="*80 + "\n")
    
    @classmethod
    def _wait_for_cooldown(cls):
        """
        ç­‰å¾…å…¨å±€å†·å´ç»“æŸ
        
        å¦‚æœå½“å‰å¤„äºå†·å´æœŸï¼Œé˜»å¡å½“å‰çº¿ç¨‹ç›´åˆ°å†·å´ç»“æŸã€‚
        æ˜¾ç¤ºå®æ—¶å€’è®¡æ—¶ã€‚
        """
        with cls._cooldown_lock:
            cooldown_until = cls._global_cooldown_until
        
        if cooldown_until is None:
            return  # æ²¡æœ‰å†·å´ï¼Œç›´æ¥è¿”å›
        
        now = datetime.now()
        if now >= cooldown_until:
            # å†·å´å·²ç»“æŸ
            with cls._cooldown_lock:
                cls._global_cooldown_until = None
            return
        
        # è¿˜åœ¨å†·å´ä¸­ï¼Œéœ€è¦ç­‰å¾…
        remaining = (cooldown_until - now).total_seconds()
        
        print(f"\nâ¸ï¸  [WAITING] å…¨å±€å†·å´ä¸­ï¼Œå‰©ä½™ {int(remaining)}ç§’...")
        
        # æ˜¾ç¤ºå€’è®¡æ—¶ï¼ˆæ¯30ç§’æ›´æ–°ä¸€æ¬¡ï¼‰
        last_print = 0
        while True:
            now = datetime.now()
            if now >= cooldown_until:
                break
            
            remaining = (cooldown_until - now).total_seconds()
            
            # æ¯30ç§’æˆ–æœ€å10ç§’æ—¶æ‰“å°æ›´æ–°
            if int(remaining) % 30 == 0 or remaining <= 10:
                if int(remaining) != last_print:
                    minutes = int(remaining) // 60
                    seconds = int(remaining) % 60
                    if minutes > 0:
                        print(f"  â³ å‰©ä½™: {minutes}åˆ†{seconds}ç§’...")
                    else:
                        print(f"  â³ å‰©ä½™: {seconds}ç§’...")
                    last_print = int(remaining)
            
            time.sleep(1)
        
        # å†·å´ç»“æŸ
        with cls._cooldown_lock:
            cls._global_cooldown_until = None
        
        print("\n" + "="*80)
        print("âœ… [COOLDOWN COMPLETE] å†·å´ç»“æŸï¼Œå®éªŒç»§ç»­")
        print("="*80 + "\n")
    
    @classmethod
    def get_cooldown_status(cls) -> dict:
        """
        è·å–å½“å‰å†·å´çŠ¶æ€ï¼ˆç”¨äºç›‘æ§ï¼‰
        
        Returns:
            dict: å†·å´çŠ¶æ€ä¿¡æ¯
        """
        with cls._cooldown_lock:
            cooldown_until = cls._global_cooldown_until
        
        if cooldown_until is None:
            return {
                'active': False,
                'remaining_seconds': 0
            }
        
        now = datetime.now()
        if now >= cooldown_until:
            return {
                'active': False,
                'remaining_seconds': 0
            }
        
        remaining = (cooldown_until - now).total_seconds()
        return {
            'active': True,
            'remaining_seconds': int(remaining),
            'ends_at': cooldown_until.strftime('%H:%M:%S')
        }
    
    def _query_without_retry(
        self,
        prompt: str = None,
        agent_name: str = None,
        role_desc: str = None,
        history_messages: List[Dict[str, str]] = None,
        global_prompt: str = None,
        request_msg: str = None,
        temperature: float = None,
        max_tokens: int = None
    ) -> str:
        """
        ç›´æ¥è°ƒç”¨APIï¼Œä¸ä½¿ç”¨retryè£…é¥°å™¨
        è¿™æ ·å¯ä»¥ç«‹å³æ•è·é”™è¯¯ï¼Œé¿å…åº•å±‚retryå¹²æ‰°å¯†é’¥åˆ‡æ¢
        
        Returns:
            str: LLM response
        """
        # Use provided temperature/max_tokens or fall back to instance defaults
        temp = temperature if temperature is not None else self.temperature
        tokens = max_tokens if max_tokens is not None else self.max_tokens
        
        # Build messages based on input format
        if prompt is not None:
            # Simple interface
            messages = [{"role": "user", "content": prompt}]
        else:
            # Complex interface (for AgentReview)
            messages = []
            
            # Add system message with role description
            if role_desc:
                messages.append({"role": "system", "content": role_desc})
            
            # Add global prompt if provided
            if global_prompt:
                messages.append({"role": "system", "content": global_prompt})
            
            # Add history messages
            if history_messages:
                for msg in history_messages:
                    messages.append(msg)
            
            # Add current request
            if request_msg:
                messages.append({"role": "user", "content": request_msg})
        
        # ç›´æ¥è°ƒç”¨OpenAI APIï¼Œä¸ä½¿ç”¨retry
        try:
            completion = self.client.chat.completions.create(
                timeout=120.0,  # 120 second timeout per request
                model=self.model,
                messages=messages,
                temperature=temp,
                max_tokens=tokens
            )
            
            # Extract response content
            content = completion.choices[0].message.content
            
            # Validate response
            if content is None:
                raise ValueError(
                    "API returned None content. This may indicate:\n"
                    "  1. Model refused to respond (content policy)\n"
                    "  2. API gateway/proxy issue\n"
                    "  3. Token limit exceeded"
                )
            
            return content
            
        except Exception as e:
            # ä¸æ‰“å°é”™è¯¯æ—¥å¿—ï¼Œè®©ä¸Šå±‚å¤„ç†
            raise
    
    def query(
        self,
        prompt: str = None,
        agent_name: str = None,
        role_desc: str = None,
        history_messages: List[Dict[str, str]] = None,
        global_prompt: str = None,
        request_msg: str = None,
        temperature: float = None,
        max_tokens: int = None,
        max_retries: int = 10
    ) -> str:
        """
        Query LLM with intelligent key rotation and retry mechanism
        
        ğŸ†• æ™ºèƒ½å¯†é’¥è½®è½¬ç­–ç•¥ (å–ä»£æ—§çš„round-robin):
        - å¼€å§‹æ—¶åªä½¿ç”¨ç¬¬1ä¸ªå¯†é’¥
        - å½“å½“å‰å¯†é’¥é‡åˆ°TPM/RPMé™æµæ—¶,ç«‹å³åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥
        - æˆåŠŸè°ƒç”¨åé‡ç½®è¿ç»­é”™è¯¯è®¡æ•°,å…è®¸ç»§ç»­ä½¿ç”¨å½“å‰å¯†é’¥
        - æ— å¯ç”¨å¯†é’¥æ—¶å¾ªç¯å›ç¬¬1ä¸ªå¯†é’¥
        
        ğŸ›¡ï¸ å››å±‚ä¿æŠ¤æœºåˆ¶:
        1. å•æ¬¡è¯·æ±‚é‡è¯• (DISABLED - ç”±æœ¬ç±»å®Œå…¨æ§åˆ¶)
        2. æŒ‡æ•°é€€é¿ (exponential backoff)
        3. æ™ºèƒ½å¯†é’¥è½®è½¬ (æœ¬æ–¹æ³•å®ç°)
        4. å…¨å±€å†·å´ (global cooldown)
        
        Compatible with OpenAIBackend interface.
        
        Args:
            max_retries: Maximum number of retry attempts (default: 10)
            Other args: See OpenAIBackend.query()
        
        Returns:
            str: LLM response text
        
        Raises:
            Exception: After all retries exhausted
        """
        # ğŸ›¡ï¸ ç¬¬å››é‡ä¿æŠ¤ï¼šæ£€æŸ¥å…¨å±€å†·å´çŠ¶æ€ï¼ˆè¿›å…¥å‰å…ˆç­‰å¾…ï¼‰
        self._wait_for_cooldown()
        
        last_exception = None
        cooldown_attempts = 0  # è¿½è¸ªå†·å´åçš„é‡è¯•æ¬¡æ•°
        MAX_COOLDOWN_CYCLES = 3  # æœ€å¤šè§¦å‘3æ¬¡å†·å´
        
        while cooldown_attempts < MAX_COOLDOWN_CYCLES:
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # æ‰§è¡Œä¸€è½®å®Œæ•´çš„é‡è¯•ï¼ˆå‰ä¸‰å±‚ä¿æŠ¤æœºåˆ¶ï¼‰
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            for attempt in range(max_retries):
                try:
                    # ğŸ†• ä½¿ç”¨æ™ºèƒ½å¯†é’¥è½®è½¬ (æ€»æ˜¯ä½¿ç”¨å½“å‰æ´»è·ƒçš„å¯†é’¥)
                    client, key_idx = self._get_current_client()
                    
                    # Log retry attempt
                    if attempt > 0 or cooldown_attempts > 0:
                        cycle_info = f" [Cooldown cycle {cooldown_attempts + 1}]" if cooldown_attempts > 0 else ""
                        print(f"[LLM Retry] Attempt {attempt + 1}/{max_retries} using {self.key_labels[key_idx]}{cycle_info}")
                    
                    # Temporarily swap client for this call
                    original_client = self.client
                    self.client = client
                    
                    try:
                        # ğŸ”§ [CRITICAL FIX] ç›´æ¥è°ƒç”¨_query_without_retryé¿å…åº•å±‚retryå¹²æ‰°
                        # è¿™æ ·å¯ä»¥ç«‹å³æ•è·429é”™è¯¯å¹¶è¿›è¡Œå¯†é’¥åˆ‡æ¢
                        result = self._query_without_retry(
                            prompt=prompt,
                            agent_name=agent_name,
                            role_desc=role_desc,
                            history_messages=history_messages,
                            global_prompt=global_prompt,
                            request_msg=request_msg,
                            temperature=temperature,
                            max_tokens=max_tokens
                        )
                        
                        # ğŸ”§ [ENHANCED] è®°å½•æˆåŠŸç»Ÿè®¡
                        self.key_stats[key_idx]['success'] += 1
                        
                        # ğŸ†• æˆåŠŸåé‡ç½®è¯¥å¯†é’¥çš„è¿ç»­é”™è¯¯è®¡æ•°
                        self._reset_consecutive_errors_on_success(key_idx)
                        
                        # Success! Log if this was a retry
                        if attempt > 0 or cooldown_attempts > 0:
                            print(f"[LLM Retry] âœ“ Success on attempt {attempt + 1} using {self.key_labels[key_idx]}")
                        
                        return result
                        
                    finally:
                        # Restore original client
                        self.client = original_client
                        
                except Exception as e:
                    last_exception = e
                    error_type = type(e).__name__
                    error_msg = str(e)
                    
                    # ğŸ”§ [ENHANCED] è®°å½•å¯†é’¥é”™è¯¯ç»Ÿè®¡
                    self.key_stats[key_idx]['failed'] += 1
                    self.key_stats[key_idx]['last_error'] = f"{error_type}: {error_msg[:100]}"
                    
                    # ğŸ†• æ™ºèƒ½å¯†é’¥è½®è½¬ï¼šæ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ‡æ¢å¯†é’¥
                    should_switch = self._should_switch_key(e, key_idx)
                    
                    # ğŸ”§ [ENHANCED] è¯¦ç»†çš„é”™è¯¯æ—¥å¿—ï¼ˆåŒ…å«å®Œæ•´çš„APIå¯†é’¥æ ‡è¯†ï¼‰
                    print(f"\n{'='*80}")
                    print(f"[LLM API ERROR] Attempt {attempt + 1}/{max_retries} failed")
                    print(f"{'='*80}")
                    print(f"ğŸ”‘ API Key: {self.key_labels[key_idx]}")
                    print(f"ğŸ·ï¸  Model: {self.model}")
                    print(f"ğŸŒ Base URL: {self.base_url or 'Official OpenAI'}")
                    print(f"âŒ Error Type: {error_type}")
                    print(f"ğŸ“ Error Message: {error_msg}")
                    
                    # ğŸ†• [DEBUG] æ˜¾ç¤ºæ‰€æœ‰å¯†é’¥çš„çŠ¶æ€
                    print(f"\nğŸ“Š å¯†é’¥çŠ¶æ€ (å…±{self.num_keys}ä¸ª):")
                    for i in range(self.num_keys):
                        status = "ğŸŸ¢ å½“å‰" if i == key_idx else "âšª å¾…å‘½"
                        errors = self._consecutive_rate_limit_errors[i]
                        print(f"   {self.key_labels[i]}: {status} (è¿ç»­é”™è¯¯: {errors}/{self._rate_limit_threshold})")
                    
                    # ğŸ†• æ˜¾ç¤ºæ˜¯å¦æ˜¯é™æµé”™è¯¯
                    if self._is_rate_limit_error(e):
                        consecutive_errors = self._consecutive_rate_limit_errors[key_idx]
                        print(f"\nâš ï¸  é™æµé”™è¯¯æ£€æµ‹: æ˜¯ (è¿ç»­{consecutive_errors}æ¬¡)")
                        if should_switch:
                            print(f"ğŸ”„ è§¦å‘å¯†é’¥è½®è½¬: è¿ç»­é™æµé”™è¯¯å·²è¾¾{self._rate_limit_threshold}æ¬¡é˜ˆå€¼")
                    else:
                        print(f"\nâŒ éé™æµé”™è¯¯ï¼Œä¸è®¡å…¥è¿ç»­é”™è¯¯è®¡æ•°")
                    
                    # ğŸ”§ [ENHANCED] ç‰¹æ®Šé”™è¯¯ç±»å‹çš„è¯¦ç»†è¯Šæ–­
                    if "401" in str(e) or "Unauthorized" in str(e):
                        print(f"\nğŸš¨ è®¤è¯å¤±è´¥ (401 Unauthorized)")
                        print(f"   é—®é¢˜å¯†é’¥: {self.key_labels[key_idx]}")
                        print(f"   å®Œæ•´å¯†é’¥: {self.api_keys[key_idx][:15]}...{self.api_keys[key_idx][-8:]}")
                        print(f"   å¯èƒ½åŸå› :")
                        print(f"     1. APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ")
                        print(f"     2. å¯†é’¥æ ¼å¼é”™è¯¯ï¼ˆå¤šä½™ç©ºæ ¼/æ¢è¡Œç¬¦ï¼‰")
                        print(f"     3. å¯†é’¥ä¸base_urlä¸åŒ¹é…")
                        print(f"   è§£å†³æ–¹æ¡ˆ:")
                        print(f"     - æ£€æŸ¥ llm_config.py ä¸­è¯¥å¯†é’¥çš„é…ç½®")
                        print(f"     - ç™»å½•APIæä¾›å•†ç¡®è®¤å¯†é’¥æœ‰æ•ˆæ€§")
                        print(f"     - å°è¯•é‡æ–°ç”Ÿæˆå¯†é’¥")
                    
                    elif "403" in str(e) or "Forbidden" in str(e):
                        print(f"\nğŸš¨ è®¿é—®è¢«æ‹’ç» (403 Forbidden)")
                        print(f"   é—®é¢˜å¯†é’¥: {self.key_labels[key_idx]}")
                        print(f"   å¯èƒ½åŸå› :")
                        print(f"     1. è´¦æˆ·ä½™é¢ä¸è¶³")
                        print(f"     2. å¯†é’¥æƒé™ä¸è¶³ï¼ˆåªè¯»å¯†é’¥ï¼‰")
                        print(f"     3. IPåœ°å€è¢«é™åˆ¶")
                        print(f"     4. æ¨¡å‹è®¿é—®æƒé™æœªå¼€é€š")
                    
                    elif "429" in str(e) or "rate limit" in error_msg.lower():
                        print(f"\nâš ï¸  é€Ÿç‡é™åˆ¶ (429 Too Many Requests)")
                        print(f"   é—®é¢˜å¯†é’¥: {self.key_labels[key_idx]}")
                        print(f"   å¯èƒ½åŸå› :")
                        print(f"     1. TPM (Tokens Per Minute) è¶…é™")
                        print(f"     2. RPM (Requests Per Minute) è¶…é™")
                        print(f"     3. æ—¥é…é¢ç”¨å°½")
                        print(f"   å½“å‰ç­–ç•¥: å°†è§¦å‘æŒ‡æ•°é€€é¿å’Œå¤šå¯†é’¥è½®è¯¢")
                    
                    elif "404" in str(e):
                        print(f"\nğŸš¨ èµ„æºä¸å­˜åœ¨ (404 Not Found)")
                        print(f"   é—®é¢˜å¯†é’¥: {self.key_labels[key_idx]}")
                        print(f"   å¯èƒ½åŸå› :")
                        print(f"     1. Base URLé”™è¯¯: {self.base_url}")
                        print(f"     2. æ¨¡å‹åç§°é”™è¯¯: {self.model}")
                        print(f"     3. APIç«¯ç‚¹è·¯å¾„é”™è¯¯")
                    
                    elif "timeout" in error_msg.lower():
                        print(f"\nâ±ï¸  è¯·æ±‚è¶…æ—¶ (Timeout)")
                        print(f"   é—®é¢˜å¯†é’¥: {self.key_labels[key_idx]}")
                        print(f"   å¯èƒ½åŸå› :")
                        print(f"     1. ç½‘ç»œè¿æ¥ä¸ç¨³å®š")
                        print(f"     2. APIæœåŠ¡å™¨å“åº”æ…¢")
                        print(f"     3. è¯·æ±‚è¿‡äºå¤æ‚ï¼ˆtokenè¿‡å¤šï¼‰")
                    
                    # Log full traceback for first and last attempts
                    if attempt == 0 or attempt == max_retries - 1:
                        print(f"\nğŸ“‹ å®Œæ•´å †æ ˆè·Ÿè¸ª:")
                        for line in traceback.format_exc().split('\n'):
                            if line.strip():
                                print(f"   {line}")
                    
                    print(f"{'='*80}\n")
                    
                    # ğŸ†• æ™ºèƒ½å¯†é’¥è½®è½¬ï¼šå¦‚æœåº”è¯¥åˆ‡æ¢å¯†é’¥,ç«‹å³åˆ‡æ¢
                    if should_switch:
                        self._switch_to_next_key(
                            failed_key_index=key_idx,
                            reason=f"è¿ç»­{self._rate_limit_threshold}æ¬¡é™æµé”™è¯¯: {error_type}"
                        )
                        # åˆ‡æ¢å¯†é’¥åç«‹å³é‡è¯•,ä¸ç­‰å¾…
                        continue
                    
                    # ğŸ›¡ï¸ ç¬¬äºŒå±‚ï¼šæŒ‡æ•°é€€é¿
                    # Don't retry immediately - exponential backoff
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 10)  # Max 10 seconds
                        print(f"            Waiting {wait_time}s before retry...")
                        time.sleep(wait_time)
                    else:
                        # All retries exhausted - log final failure
                        print(f"\n{'='*80}")
                        print(f"[LLM ERROR] âœ— All {max_retries} attempts failed!")
                        print(f"{'='*80}")
                        print(f"Final error: {error_type}: {error_msg}")
                        print(f"\nğŸ“Š APIå¯†é’¥ä½¿ç”¨ç»Ÿè®¡:")
                        for i, stats in enumerate(self.key_stats):
                            total = stats['success'] + stats['failed']
                            success_rate = (stats['success'] / total * 100) if total > 0 else 0
                            print(f"  {self.key_labels[i]}:")
                            print(f"    æˆåŠŸ: {stats['success']}, å¤±è´¥: {stats['failed']}, æˆåŠŸç‡: {success_rate:.1f}%")
                            if stats['last_error']:
                                print(f"    æœ€åé”™è¯¯: {stats['last_error']}")
                        print(f"{'='*80}\n")
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # ğŸ›¡ï¸ ç¬¬å››é‡ä¿æŠ¤ï¼šå‰ä¸‰å±‚å…¨éƒ¨å¤±è´¥åçš„åˆ¤æ–­
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            
            # æ‰€æœ‰10æ¬¡é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘å†·å´
            if last_exception and self._should_trigger_cooldown(last_exception):
                cooldown_attempts += 1
                
                print(f"\n{'='*80}")
                print(f"âš ï¸  å‰ä¸‰å±‚ä¿æŠ¤æœºåˆ¶å·²ç”¨å°½ï¼ˆ10æ¬¡é‡è¯• + 75ç§’ç­‰å¾…ï¼‰")
                print(f"   æœ€åçš„é”™è¯¯ç±»å‹: {type(last_exception).__name__}")
                print(f"   é”™è¯¯ä¿¡æ¯: {str(last_exception)[:100]}")
                
                if cooldown_attempts < MAX_COOLDOWN_CYCLES:
                    print(f"   â†’ å¯åŠ¨ç¬¬å››å±‚ä¿æŠ¤ï¼šå…¨å±€å†·å´ ({cooldown_attempts}/{MAX_COOLDOWN_CYCLES})")
                    print(f"{'='*80}\n")
                    
                    # è§¦å‘å…¨å±€å†·å´
                    self._trigger_global_cooldown(
                        reason=f"{type(last_exception).__name__}: {str(last_exception)[:100]}"
                    )
                    
                    # ç­‰å¾…å†·å´ç»“æŸ
                    self._wait_for_cooldown()
                    
                    print(f"\n{'='*80}")
                    print(f"ğŸ”„ å†·å´å®Œæˆï¼Œå¼€å§‹æ–°ä¸€è½®é‡è¯•ï¼ˆå°†å†æ¬¡æ‰§è¡Œ10æ¬¡é‡è¯•ï¼‰")
                    print(f"{'='*80}\n")
                    
                    # ç»§ç»­å¤–å±‚whileå¾ªç¯ï¼Œå¼€å§‹æ–°ä¸€è½®é‡è¯•
                    continue
                else:
                    print(f"   âš ï¸  å·²è¾¾åˆ°æœ€å¤§å†·å´æ¬¡æ•° ({MAX_COOLDOWN_CYCLES}æ¬¡)")
                    print(f"   â†’ æ”¾å¼ƒé‡è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸")
                    print(f"{'='*80}\n")
                    break
            else:
                # ä¸æ˜¯rate limitç±»å‹çš„é”™è¯¯ï¼Œæˆ–è€…æ²¡æœ‰é”™è¯¯
                # ç›´æ¥é€€å‡ºï¼Œä¸è§¦å‘å†·å´
                break
        
        # æ‰€æœ‰å°è¯•ï¼ˆåŒ…æ‹¬å†·å´åçš„é‡è¯•ï¼‰éƒ½å¤±è´¥äº†
        if last_exception is not None:
            raise last_exception
        else:
            raise RuntimeError("All retry attempts failed but no exception was captured")
    
    def get_stats(self) -> Dict:
        """Get usage statistics"""
        with self._lock:
            current_key = self._current_key_index
            consecutive_errors = self._consecutive_rate_limit_errors.copy()
        
        return {
            "num_keys": self.num_keys,
            "keys_preview": [key[:10] + "..." for key in self.api_keys],
            "key_labels": self.key_labels,
            "expected_throughput_boost": f"{self.num_keys}x",
            "key_stats": self.key_stats,
            "current_active_key": current_key,
            "current_active_key_label": self.key_labels[current_key],
            "consecutive_errors_per_key": consecutive_errors,
            "rotation_threshold": self._rate_limit_threshold,
        }
    
    def print_key_stats(self):
        """
        æ‰“å°è¯¦ç»†çš„APIå¯†é’¥ä½¿ç”¨ç»Ÿè®¡
        ç”¨äºå®éªŒç»“æŸåçš„æ€»ç»“
        """
        print(f"\n{'='*80}")
        print(f"ğŸ“Š APIå¯†é’¥ä½¿ç”¨ç»Ÿè®¡æ€»ç»“")
        print(f"{'='*80}")
        
        # è·å–å½“å‰çŠ¶æ€
        stats_info = self.get_stats()
        current_key = stats_info["current_active_key"]
        
        total_success = 0
        total_failed = 0
        
        for i, stats in enumerate(self.key_stats):
            total = stats['success'] + stats['failed']
            total_success += stats['success']
            total_failed += stats['failed']
            success_rate = (stats['success'] / total * 100) if total > 0 else 0
            
            # æ ‡è®°å½“å‰æ´»è·ƒå¯†é’¥
            active_marker = " ğŸŸ¢ [å½“å‰æ´»è·ƒ]" if i == current_key else ""
            consecutive = stats_info["consecutive_errors_per_key"][i]
            
            print(f"\n{self.key_labels[i]}{active_marker}:")
            print(f"  âœ… æˆåŠŸè°ƒç”¨: {stats['success']} æ¬¡")
            print(f"  âŒ å¤±è´¥è°ƒç”¨: {stats['failed']} æ¬¡")
            print(f"  ğŸ“ˆ æˆåŠŸç‡: {success_rate:.2f}%")
            print(f"  ğŸ“Š æ€»è°ƒç”¨: {total} æ¬¡")
            print(f"  ğŸ”„ è¿ç»­é™æµé”™è¯¯: {consecutive} æ¬¡ (é˜ˆå€¼: {self._rate_limit_threshold})")
            
            if stats['last_error']:
                print(f"  ğŸ”´ æœ€åé”™è¯¯: {stats['last_error']}")
        
        # æ€»ä½“ç»Ÿè®¡
        overall_total = total_success + total_failed
        overall_success_rate = (total_success / overall_total * 100) if overall_total > 0 else 0
        
        print(f"\n{'â”€'*80}")
        print(f"æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»æˆåŠŸ: {total_success} æ¬¡")
        print(f"  æ€»å¤±è´¥: {total_failed} æ¬¡")
        print(f"  æ€»æˆåŠŸç‡: {overall_success_rate:.2f}%")
        print(f"  æ€»è°ƒç”¨: {overall_total} æ¬¡")
        print(f"\nå¯†é’¥è½®è½¬ç­–ç•¥:")
        print(f"  æ¨¡å¼: æ™ºèƒ½è½®è½¬ (åŸºäºé™æµé”™è¯¯)")
        print(f"  åˆ‡æ¢é˜ˆå€¼: è¿ç»­{self._rate_limit_threshold}æ¬¡é™æµé”™è¯¯")
        print(f"  å½“å‰æ´»è·ƒå¯†é’¥: {self.key_labels[current_key]}")
        print(f"{'='*80}\n")


def create_multi_key_backend(
    model_name: str = "deepseek-ai/DeepSeek-V3.2-Exp",
    temperature: float = 0.6,
    max_tokens: int = 700
) -> MultiKeyOpenAIBackend:
    """
    Convenient factory function to create multi-key backend
    
    Usage:
        # Set environment variables first:
        # export OPENAI_API_KEY_1="sk-xxx-account1"
        # export OPENAI_API_KEY_2="sk-xxx-account2"
        
        backend = create_multi_key_backend()
    """
    return MultiKeyOpenAIBackend(
        api_keys=None,  # Auto-detect
        model_name=model_name,
        temperature=temperature,
        max_tokens=max_tokens
    )


if __name__ == "__main__":
    print("Multi-Key Backend Configuration Test")
    print("=" * 80)
    
    # Test auto-detection
    try:
        backend = create_multi_key_backend()
        stats = backend.get_stats()
        
        print("\nâœ“ Configuration successful!")
        print(f"  - Number of keys: {stats['num_keys']}")
        print(f"  - Keys preview: {stats['keys_preview']}")
        print(f"  - Expected boost: {stats['expected_throughput_boost']}")
        
    except ValueError as e:
        print(f"\nâœ— Configuration failed:")
        print(f"  {e}")
        print("\nPlease set environment variables:")
        print("  export OPENAI_API_KEY_1='your-first-key'")
        print("  export OPENAI_API_KEY_2='your-second-key'")

