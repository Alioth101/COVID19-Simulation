"""
OpenAI backend implementation
Adapted from AgentReview project
"""

import os
import json
import re
from typing import List, Dict
from tenacity import retry, stop_after_attempt, wait_random_exponential

from .base import IntelligenceBackend


class OpenAIBackend(IntelligenceBackend):
    """
    OpenAI API backend
    
    Supports:
    - Standard OpenAI API
    - Azure OpenAI
    - Third-party OpenAI-compatible APIs (OpenRouter, DeepSeek, etc.)
    """
    
    def __init__(
        self,
        model_name: str = "gpt-4o-mini",
        temperature: float = 0.7,
        max_tokens: int = 2000,
        api_key: str = None,
        base_url: str = None,
        azure_endpoint: str = None,
        azure_deployment: str = None
    ):
        """
        Initialize OpenAI backend
        
        Args:
            model_name: Model to use (e.g., "gpt-4o-mini", "gpt-4o", "deepseek-chat")
            temperature: Sampling temperature (0.0-2.0)
            max_tokens: Max response tokens
            api_key: API key (OpenAI/Azure/third-party)
            base_url: Custom API base URL (for third-party providers)
                Examples:
                - OpenRouter: "https://openrouter.ai/api/v1"
                - DeepSeek: "https://api.deepseek.com"
                - Local proxy: "http://localhost:8000/v1"
            azure_endpoint: Azure OpenAI endpoint (for Azure only)
            azure_deployment: Azure deployment name (for Azure only)
        
        Environment variables (auto-detected if not provided):
            - OPENAI_API_KEY: API key
            - OPENAI_BASE_URL: Base URL
            - AZURE_ENDPOINT: Azure endpoint
            - AZURE_DEPLOYMENT: Azure deployment
        """
        super().__init__(model_name, temperature, max_tokens)
        
        # Get configuration from environment if not provided
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url or os.getenv("OPENAI_BASE_URL")
        self.azure_endpoint = azure_endpoint or os.getenv("AZURE_ENDPOINT")
        self.azure_deployment = azure_deployment or os.getenv("AZURE_DEPLOYMENT")
        
        if not self.api_key:
            raise ValueError(
                "API key not provided. Please either:\n"
                "  1. Set OPENAI_API_KEY environment variable, or\n"
                "  2. Pass api_key parameter when creating backend"
            )
        
        # Initialize OpenAI client
        self._init_client()
    
    def _init_client(self):
        """Initialize OpenAI, Azure OpenAI, or third-party client"""
        try:
            from openai import OpenAI, AzureOpenAI
        except ImportError:
            raise ImportError(
                "openai package not installed. Run: pip install openai>=1.0.0"
            )
        
        if self.azure_endpoint:
            # Azure OpenAI
            print(f"[OpenAI Backend] Using Azure OpenAI: {self.azure_endpoint}")
            self.client = AzureOpenAI(
                api_key=self.api_key,
                api_version="2024-02-15-preview",
                azure_endpoint=self.azure_endpoint
            )
            self.model = self.azure_deployment
        
        elif self.base_url:
            # Third-party OpenAI-compatible API
            print(f"[OpenAI Backend] Using custom base URL: {self.base_url}")
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            self.model = self.model_name
        
        else:
            # Standard OpenAI API
            print(f"[OpenAI Backend] Using standard OpenAI API")
            self.client = OpenAI(api_key=self.api_key)
            self.model = self.model_name
    
    def query(
        self,
        prompt: str = None,
        agent_name: str = None,
        role_desc: str = None,
        history_messages: List[Dict[str, str]] = None,
        global_prompt: str = None,
        request_msg: str = None,
        temperature: float = None,
        max_tokens: int = None
    ) -> str:
        """
        Query OpenAI API
        
        Supports two interfaces:
        1. Simple interface (for GraphSimulation):
           query(prompt, temperature=0.7, max_tokens=300)
        
        2. Complex interface (for AgentReview):
           query(agent_name, role_desc, history_messages, global_prompt, request_msg)
        
        Returns:
            str: LLM response
        """
        # Use provided temperature/max_tokens or fall back to instance defaults
        temp = temperature if temperature is not None else self.temperature
        tokens = max_tokens if max_tokens is not None else self.max_tokens
        
        # Construct messages
        messages = []
        
        # Interface 1: Simple prompt-based (for GraphSimulation)
        if prompt:
            messages.append({"role": "user", "content": prompt})
        
        # Interface 2: Complex multi-part (for AgentReview)
        else:
            # System message: combine global prompt + role description
            system_content = f"{global_prompt}\n\n{role_desc}" if global_prompt else role_desc
            messages.append({"role": "system", "content": system_content})
            
            # Add history (recent decisions for context)
            if history_messages:
                for hist in history_messages[-3:]:  # Only keep last 3 for context
                    messages.append({
                        "role": "assistant",
                        "content": hist.get("content", "")
                    })
            
            # Add current request
            messages.append({"role": "user", "content": request_msg})
        
        # Call API with retry
        response = self._get_response_with_retry(messages, temp, tokens)
        return response
    
    @retry(stop=stop_after_attempt(3), wait=wait_random_exponential(min=1, max=10))
    def _get_response_with_retry(self, messages: List[Dict], temperature: float, max_tokens: int) -> str:
        """
        Call OpenAI API with retry logic
        
        Args:
            messages: List of message dicts
            temperature: Sampling temperature
            max_tokens: Max response tokens
            
        Returns:
            str: Response content
        """
        try:
            completion = self.client.chat.completions.create(
                timeout=120.0,  # 120 second timeout per request
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # Extract response content
            content = completion.choices[0].message.content
            
            # Validate response
            if content is None:
                raise ValueError(
                    "API returned None content. This may indicate:\n"
                    "  1. Model refused to respond (content policy)\n"
                    "  2. API gateway/proxy issue\n"
                    "  3. Token limit exceeded"
                )
            
            return content
            
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            
            # ğŸ”§ [ENHANCED] è¯¦ç»†çš„é”™è¯¯è¯Šæ–­
            print(f"\n{'='*80}")
            print(f"[OpenAI API ERROR] Request failed")
            print(f"{'='*80}")
            print(f"ğŸ”‘ API Key: {self.api_key[:10]}...{self.api_key[-4:]}")
            print(f"ğŸ·ï¸  Model: {self.model}")
            print(f"ğŸŒ Base URL: {getattr(self, 'base_url', 'Official OpenAI API')}")
            print(f"âŒ Error Type: {error_type}")
            print(f"ğŸ“ Error Message: {error_msg}")
            
            # ç‰¹æ®Šé”™è¯¯ç±»å‹è¯Šæ–­
            if "404" in str(e):
                print(f"\nğŸš¨ èµ„æºä¸å­˜åœ¨ (404 Not Found)")
                print(f"   å¯èƒ½åŸå› :")
                print(f"     1. Base URLé”™è¯¯: {getattr(self, 'base_url', 'N/A')}")
                print(f"     2. æ¨¡å‹åç§°ä¸å¯ç”¨: {self.model}")
                print(f"     3. APIç«¯ç‚¹è·¯å¾„é”™è¯¯ (æ£€æŸ¥æ˜¯å¦éœ€è¦ /v1 åç¼€)")
                print(f"   è§£å†³æ–¹æ¡ˆ:")
                print(f"     - ä½¿ç”¨å®˜æ–¹OpenAI: åˆ é™¤ OPENAI_BASE_URL ç¯å¢ƒå˜é‡")
                print(f"     - OpenRouter: base_url='https://openrouter.ai/api/v1'")
                print(f"     - DeepSeek: base_url='https://api.deepseek.com'")
            
            elif "401" in str(e) or "Unauthorized" in str(e):
                print(f"\nğŸš¨ è®¤è¯å¤±è´¥ (401 Unauthorized)")
                print(f"   é—®é¢˜å¯†é’¥: {self.api_key[:15]}...{self.api_key[-8:]}")
                print(f"   å®Œæ•´å¯†é’¥ï¼ˆç”¨äºæ£€æŸ¥ï¼‰: {self.api_key}")
                print(f"   å¯èƒ½åŸå› :")
                print(f"     1. APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ")
                print(f"     2. å¯†é’¥æ ¼å¼é”™è¯¯ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰å¤šä½™ç©ºæ ¼/æ¢è¡Œç¬¦ï¼‰")
                print(f"     3. å¯†é’¥ä¸base_urlä¸åŒ¹é…")
                print(f"   è§£å†³æ–¹æ¡ˆ:")
                print(f"     - æ£€æŸ¥ç¯å¢ƒå˜é‡æˆ–llm_config.pyä¸­çš„é…ç½®")
                print(f"     - ç™»å½•APIæä¾›å•†ç¡®è®¤å¯†é’¥æœ‰æ•ˆæ€§")
                print(f"     - å°è¯•é‡æ–°ç”Ÿæˆå¯†é’¥")
            
            elif "403" in str(e) or "Forbidden" in str(e):
                print(f"\nğŸš¨ è®¿é—®è¢«æ‹’ç» (403 Forbidden)")
                print(f"   å¯èƒ½åŸå› :")
                print(f"     1. è´¦æˆ·ä½™é¢ä¸è¶³")
                print(f"     2. å¯†é’¥æƒé™ä¸è¶³ï¼ˆå¯èƒ½æ˜¯åªè¯»å¯†é’¥ï¼‰")
                print(f"     3. IPåœ°å€è¢«é™åˆ¶")
                print(f"     4. æ¨¡å‹è®¿é—®æƒé™æœªå¼€é€šï¼ˆå¦‚GPT-4éœ€è¦ç‰¹æ®Šæƒé™ï¼‰")
                print(f"   è§£å†³æ–¹æ¡ˆ:")
                print(f"     - æ£€æŸ¥è´¦æˆ·ä½™é¢")
                print(f"     - ç”Ÿæˆå…·æœ‰å®Œæ•´æƒé™çš„æ–°å¯†é’¥")
                print(f"     - è”ç³»APIæä¾›å•†ç¡®è®¤è®¿é—®æƒé™")
            
            elif "429" in str(e):
                print(f"\nâš ï¸  é€Ÿç‡é™åˆ¶ (429 Too Many Requests)")
                print(f"   å¯èƒ½åŸå› :")
                print(f"     1. TPM (Tokens Per Minute) è¶…é™")
                print(f"     2. RPM (Requests Per Minute) è¶…é™")
                print(f"     3. æ—¥é…é¢ç”¨å°½")
                print(f"   å»ºè®®:")
                print(f"     - ä½¿ç”¨ MultiKeyOpenAIBackend è¿›è¡Œå¤šå¯†é’¥è´Ÿè½½å‡è¡¡")
                print(f"     - é™ä½ MAX_CONCURRENT_LLM å‚æ•°")
                print(f"     - ç­‰å¾…é€Ÿç‡é™åˆ¶çª—å£é‡ç½®")
            
            elif "timeout" in error_msg.lower():
                print(f"\nâ±ï¸  è¯·æ±‚è¶…æ—¶ (Timeout)")
                print(f"   å¯èƒ½åŸå› :")
                print(f"     1. ç½‘ç»œè¿æ¥ä¸ç¨³å®š")
                print(f"     2. APIæœåŠ¡å™¨å“åº”æ…¢")
                print(f"     3. è¯·æ±‚è¿‡äºå¤æ‚ï¼ˆprompt tokensè¿‡å¤šï¼‰")
                print(f"   å½“å‰è¶…æ—¶è®¾ç½®: 120ç§’")
            
            elif "connection" in error_msg.lower():
                print(f"\nğŸŒ ç½‘ç»œè¿æ¥é”™è¯¯")
                print(f"   å¯èƒ½åŸå› :")
                print(f"     1. æ— æ³•è®¿é—®APIæœåŠ¡å™¨")
                print(f"     2. é˜²ç«å¢™/ä»£ç†é˜»æ­¢è¿æ¥")
                print(f"     3. DNSè§£æå¤±è´¥")
                print(f"   è§£å†³æ–¹æ¡ˆ:")
                print(f"     - æ£€æŸ¥ç½‘ç»œè¿æ¥")
                print(f"     - éªŒè¯base_urlå¯è®¿é—®æ€§")
                print(f"     - æ£€æŸ¥ä»£ç†è®¾ç½®")
            
            print(f"{'='*80}\n")
            raise
