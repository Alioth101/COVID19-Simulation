"""
LLM-based Graph Simulation Batch Experiment

This script runs batch experiments for graph-based network simulations with LLM decision-making.
It uses the standard batch_experiment function and generates ABS-compatible CSV output.

Key features:
- Graph-based network simulation (GraphSimulation class)
- LLM-powered agent decision making
- Network structure with Business, House, and Person agents
- Standard CSV output format
- Optional LLM decision logging
"""

# ============================================================================
# ğŸ›ï¸ EXPERIMENT CONFIGURATION PANEL
# ============================================================================
# Easily toggle different logging and debugging features for your experiments
# ============================================================================

# ğŸ“Š ESSENTIAL OUTPUTS (Always Generated)
# ----------------------------------------
# âœ… resultsP500.csv         - Main experiment results (ALWAYS GENERATED)
# âœ… llm_decisionsP500.json  - LLM decision logs (if ENABLE_LLM_LOGGING=True)

# ğŸ”§ DEBUG OUTPUTS (Optional)
# ----------------------------------------
ENABLE_LLM_LOGGING = True          # ğŸ“ LLM decision logs (llm_decisionsP500.json)
ENABLE_ECONOMIC_DEBUG = False      # ğŸ’° Economic transaction logs (economic_debug_*.json)
ENABLE_CONSOLE_OUTPUT = False      # ğŸ“‹ Console output logs (console_output_*.log)
ENABLE_CASHFLOW_DEBUG = False      # ğŸ’¸ Cashflow debug logs (debug_cashflow.log)
ENABLE_HIRING_DEBUG = False        # ğŸ‘¥ Business hiring debug prints
ENABLE_GOVERNMENT_DEBUG = False    # ğŸ›ï¸ Government accounting debug prints
ENABLE_WEALTH_SNAPSHOTS = False    # ğŸ“¸ Wealth conservation snapshots

# ğŸ¤– LLM CONFIGURATION
# ----------------------------------------
MAX_TOKENS = 1200                  # ğŸ“ Max LLM output length (1200-1500 recommended)
ENABLE_LLM_PROGRESS = True         # ğŸ“Š Show LLM decision progress bar

# ğŸ¯ QUICK PRESETS
# ----------------------------------------
# Uncomment one of these presets for common configurations:

# # ğŸƒ PRODUCTION MODE (Minimal logs, fast execution)
# ENABLE_LLM_LOGGING = True
# ENABLE_ECONOMIC_DEBUG = False
# ENABLE_CONSOLE_OUTPUT = False
# ENABLE_CASHFLOW_DEBUG = False
# ENABLE_HIRING_DEBUG = False
# ENABLE_GOVERNMENT_DEBUG = False
# ENABLE_WEALTH_SNAPSHOTS = False
# ENABLE_LLM_PROGRESS = True

# # ğŸ› DEBUG MODE (All logs enabled)
# ENABLE_LLM_LOGGING = True
# ENABLE_ECONOMIC_DEBUG = True
# ENABLE_CONSOLE_OUTPUT = True
# ENABLE_CASHFLOW_DEBUG = True
# ENABLE_HIRING_DEBUG = True
# ENABLE_GOVERNMENT_DEBUG = True
# ENABLE_WEALTH_SNAPSHOTS = True
# ENABLE_LLM_PROGRESS = True

# # ğŸ” ECONOMIC DEBUG (Focus on economic issues)
# ENABLE_LLM_LOGGING = True
# ENABLE_ECONOMIC_DEBUG = True
# ENABLE_CONSOLE_OUTPUT = False
# ENABLE_CASHFLOW_DEBUG = True
# ENABLE_HIRING_DEBUG = False
# ENABLE_GOVERNMENT_DEBUG = True
# ENABLE_WEALTH_SNAPSHOTS = True
# ENABLE_LLM_PROGRESS = False

# ============================================================================

import os
import sys
import numpy as np
import random

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Also add script directory to path (for llm_config.py)
script_dir = os.path.dirname(os.path.abspath(__file__))
if script_dir not in sys.path:
    sys.path.insert(0, script_dir)

from covid_abs.network.graph_abs import GraphSimulation
from covid_abs.experiments import batch_experiment
from covid_abs.llm.openai_backend import OpenAIBackend
from covid_abs.agents import Status
from covid_abs.economic_logger import economic_logger


def main():
    """
    Run batch experiments for graph-based LLM simulation
    """
    
    # 
    RANDOM_SEED = 42
    np.random.seed(RANDOM_SEED)
    random.seed(RANDOM_SEED)
    print(f"Random seed set to: {RANDOM_SEED} (for reproducibility)\n")
    
    # ============================================================================
    # [CONFIG] 
    # ============================================================================
    print("\n" + "="*80)
    print("  Graph-Based Network LLM Batch Simulation")
    print("  (llm_config.py)")
    print("="*80 + "\n")
    
    # 
    backend = None
    max_concurrent_llm = 3  # ,llm_config.py
    
    try:
        from llm_config import get_llm_config
        
        llm_config = get_llm_config()
        configs = llm_config["configs"]
        params = llm_config["parameters"]
        
        print(f"âœ… ä» llm_config.py åŠ è½½é…ç½®æˆåŠŸ")
        print(f"   æœ‰æ•ˆAPIé…ç½®: {len(configs)}ç»„")
        for i, cfg in enumerate(configs, 1):
            print(f"   {i}. {cfg['name']}: {cfg['api_key'][:10]}...{cfg['api_key'][-4:]}")
        print()
        
        # ä½¿ç”¨MultiKeyOpenAIBackend(æ”¯æŒè‡ªåŠ¨è´Ÿè½½å‡è¡¡)
        try:
            from covid_abs.llm.multi_key_backend import MultiKeyOpenAIBackend
            
            backend = MultiKeyOpenAIBackend(
                api_keys=[cfg["api_key"] for cfg in configs],
                model_name=configs[0]["model"],  # ä½¿ç”¨ç¬¬1ç»„çš„model
                temperature=params["temperature"],
                max_tokens=MAX_TOKENS,  # âœ… ä½¿ç”¨å®éªŒé…ç½®çš„max_tokens
                base_url=configs[0]["base_url"],  # ä½¿ç”¨ç¬¬1ç»„çš„base_url
                proxies=[cfg.get("proxy") for cfg in configs]  # âœ… ä¸ºæ¯ä¸ªå¯†é’¥é…ç½®ä»£ç†
            )
            
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å¹¶å‘æ•°(è¦†ç›–é»˜è®¤å€¼)
            max_concurrent_llm = params.get("max_concurrent") or max_concurrent_llm
            
            print(f"[CONFIG] LLM Decision Logging: {'ENABLED âœ“' if ENABLE_LLM_LOGGING else 'DISABLED'}")
            print(f"[CONFIG] Max Tokens: {MAX_TOKENS} (ç”¨äºæ§åˆ¶LLMå“åº”é•¿åº¦)")
            print(f"[CONFIG] å¹¶å‘æ•°: {max_concurrent_llm} (æ¨è: {len(configs)}Ã—8 = {len(configs)*8})")
            print(f"[CONFIG] é¢„æœŸé€Ÿåº¦æå‡: {len(configs)}x")
            print()
            
        except ImportError as e:
            print(f"âŒ å¯¼å…¥MultiKeyOpenAIBackendå¤±è´¥: {e}")
            print("   æ­£åœ¨å›é€€åˆ°å•å¯†é’¥æ¨¡å¼...")
            print()
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªé…ç½®åˆ›å»ºå•å¯†é’¥backend
            backend = OpenAIBackend(
                model_name=configs[0]["model"],
                temperature=params["temperature"],
                max_tokens=MAX_TOKENS,
                api_key=configs[0]["api_key"],
                base_url=configs[0]["base_url"]
            )
            
            print(f"âœ… ä½¿ç”¨å•å¯†é’¥æ¨¡å¼ (API Key: {configs[0]['name']})")
            print(f"[CONFIG] LLM Decision Logging: {'ENABLED âœ“' if ENABLE_LLM_LOGGING else 'DISABLED'}")
            print(f"[CONFIG] Max Tokens: {MAX_TOKENS}")
            print(f"[CONFIG] Max Concurrent LLM Calls: {max_concurrent_llm}")
            print()
        
    except ImportError as ie:
        print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° llm_config.py ({str(ie)})")
        print("   ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®")
        print()
        
        # å›é€€åˆ°ç¯å¢ƒå˜é‡æ¨¡å¼
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("ERROR: OPENAI_API_KEY environment variable not set!")
            print("è¯·é…ç½® llm_config.py æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
            return
        
        backend = OpenAIBackend(
            model_name="deepseek-ai/DeepSeek-V3.2-Exp",
            temperature=0.6,
            max_tokens=MAX_TOKENS,  # âœ… ä½¿ç”¨å®éªŒé…ç½®çš„max_tokens
            api_key=api_key
        )
        # max_concurrent_llm ä½¿ç”¨å·²è®¾ç½®çš„é»˜è®¤å€¼3
        print(f"âœ… ä½¿ç”¨å•å¯†é’¥æ¨¡å¼ (ä»ç¯å¢ƒå˜é‡)")
        print(f"[CONFIG] Max Concurrent LLM Calls: {max_concurrent_llm}")
        print()
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        print(f"   è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
        print()
        print("è¯·æ£€æŸ¥ llm_config.py é…ç½®:")
        print("  python llm_config.py")
        return
    
    # Ensure backend is created
    if backend is None:
        print("ERROR: Unable to create LLM backend!")
        return
    
    # Experiment parameters
    experiments = 3
    iterations = 1488  # 62 days (2 months)
    population_size = 50
    
    print("Configuration:")
    print(f"  - Experiments: {experiments}")
    print(f"  - Iterations: {iterations} ({iterations//24} days)")
    print(f"  - Population Size: {population_size} agents")
    print(f"  - Initial Infected: 10%")
    print(f"  - Network Type: Graph-based (Business-House-Person)")
    print()
    
    # Create output directory (use absolute path based on script location)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "output", "graph_batch")
    os.makedirs(output_dir, exist_ok=True)
    
    # Setup console output logging (if enabled)
    import sys
    from datetime import datetime
    
    tee = None
    if ENABLE_CONSOLE_OUTPUT:
        console_log_file = os.path.join(output_dir, f"console_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        
        class TeeOutput:
            """Tee output to both console and file"""
            def __init__(self, file_path):
                self.terminal = sys.stdout
                self.log_file = open(file_path, 'w', encoding='utf-8')
            
            def write(self, message):
                self.terminal.write(message)
                self.log_file.write(message)
                self.log_file.flush()  # Ensure immediate write
            
            def flush(self):
                self.terminal.flush()
                self.log_file.flush()
            
            def close(self):
                self.log_file.close()
        
        # Redirect stdout to both console and file
        tee = TeeOutput(console_log_file)
        sys.stdout = tee
        
        print(f"ğŸ“ Console output will be saved to: {console_log_file}")
    
    # Apply debug configurations
    from covid_abs.network import log_config
    log_config.DEBUG_CASHFLOW = ENABLE_CASHFLOW_DEBUG
    
    # Set debug flags in environment for agents.py to read
    os.environ['ENABLE_HIRING_DEBUG'] = str(ENABLE_HIRING_DEBUG)
    os.environ['ENABLE_GOVERNMENT_DEBUG'] = str(ENABLE_GOVERNMENT_DEBUG)
    os.environ['ENABLE_WEALTH_SNAPSHOTS'] = str(ENABLE_WEALTH_SNAPSHOTS)
    os.environ['ENABLE_LLM_PROGRESS'] = str(ENABLE_LLM_PROGRESS)
    
    # Prepare output files
    csv_file = os.path.join(output_dir, "resultsP500.csv")
    llm_log_file = os.path.join(output_dir, "llm_decisionsP500.json") if ENABLE_LLM_LOGGING else None
    
    # ============================================================================
    # ğŸ¬ SCENARIO CONFIGURATION - Government Policy System
    # ============================================================================
    # Control how Government makes policy decisions
    # ============================================================================
    
    SCENARIO_CONFIG = {
        # ============================================================================
        # [CONFIG] AGENT DECISION FREQUENCIES (hours between decisions)
        # ============================================================================
        'person_decision_interval': 2,      # Personå†³ç­–é¢‘ç‡ ï¼ˆdefault: 6ï¼‰
        'business_decision_interval': 24,   # Businesså†³ç­–é¢‘ç‡ ï¼ˆdefault: 12ï¼‰
        'government_decision_interval':48,  # Governmentå†³ç­–é¢‘ç‡ ï¼ˆdefault: 24ï¼‰
        
        # ============================================================================
        # [CONFIG] POLICY RECOMMENDATION SYSTEM (Dynamic prompt based on epidemic situation)
        # ============================================================================
        'enable_policy_recommendation': True,  # ğŸ‘ˆ Set False to disable dynamic prompts
        'policy_recommendation_mode': 'balanced',  # Options: 'aggressive', 'balanced', 'conservative'
        
        # [CONFIG] Infection Rate Thresholds (for policy recommendations)
        'critical_threshold': 0.10,    # >10% = Critical situation
        'high_threshold': 0.05,        # >5% = High risk
        'moderate_threshold': 0.02,    # >2% = Moderate risk
        
        # [CONFIG] Economic Weight (how much to emphasize economic factors in recommendations)
        'economic_weight': 0.5,  # 0.0=health only, 1.0=economy only, 0.5=balanced
    }
    
    print("\n" + "="*80)
    print("  ğŸ¬ SCENARIO CONFIGURATION")
    print("="*80)
    print(f"\n  ğŸ“… Agent Decision Frequencies:")
    print(f"    Person:     {SCENARIO_CONFIG['person_decision_interval']} hours ({SCENARIO_CONFIG['person_decision_interval']/24:.2f} days)")
    print(f"    Business:   {SCENARIO_CONFIG['business_decision_interval']} hours ({SCENARIO_CONFIG['business_decision_interval']/24:.2f} days)")
    print(f"    Government: {SCENARIO_CONFIG['government_decision_interval']} hours ({SCENARIO_CONFIG['government_decision_interval']/24:.2f} days)")
    
    print(f"\n  ğŸ›ï¸ Policy Recommendation: {'ENABLED âœ“' if SCENARIO_CONFIG['enable_policy_recommendation'] else 'DISABLED'}")
    if SCENARIO_CONFIG['enable_policy_recommendation']:
        print(f"    - Mode: {SCENARIO_CONFIG['policy_recommendation_mode']}")
        print(f"    - Critical Threshold: {SCENARIO_CONFIG['critical_threshold']*100}%")
        print(f"    - High Threshold: {SCENARIO_CONFIG['high_threshold']*100}%")
        print(f"    - Economic Weight: {SCENARIO_CONFIG['economic_weight']}")
    print()
    
    # Economic configuration
    total_wealth = 1800000      # [CONFIG] åŸºç¡€ç»æµæ€»é‡ (é»˜è®¤: 10000)
    business_gdp_share = 0.4     # [CONFIG] Businessè´¢å¯Œå æ¯” (é»˜è®¤: 0.5)
    public_gdp_share = 0.1       # [CONFIG] Governmentè´¢å¯Œå æ¯” (é»˜è®¤: 0.1)
    minimum_income = 900.0        # [CONFIG] æœ€ä½æ”¶å…¥
    minimum_expense = 600.0       # [CONFIG] æœ€ä½æ”¯å‡º
    
    # [CONFIG] ç»æµå¼€æ”¾åº¦é…ç½® (0.0=å®Œå…¨å°é—­, 1.0=å®Œå…¨å¼€æ”¾)
    ECONOMY_PRESETS = {
        'closed': 0.0,      # å®Œå…¨å°é—­ç»æµ
        'moderate': 0.3,    # é€‚åº¦å¼€æ”¾
        'balanced': 0.5,    # å¹³è¡¡å¼€æ”¾
        'open': 1.0         # å®Œå…¨å¼€æ”¾
    }
    economy_openness = ECONOMY_PRESETS['moderate']  # [CONFIG] æˆ–ç›´æ¥è®¾ç½®æ•°å€¼ å¦‚ 0.3
    
    # [CONFIG] Businessè¿è¥æˆæœ¬ç³»æ•°é…ç½®
    # ========================================
    # è¯´æ˜ï¼šBusinessè¿è¥æˆæœ¬ = å‘˜å·¥æ•°é‡ Ã— é˜¶å±‚ç³»æ•°
    # - åŸºç¡€æˆæœ¬ï¼šæ‰€æœ‰é˜¶å±‚çš„åŸºå‡†å€¼ï¼ˆå…ƒ/å‘˜å·¥/æœˆï¼‰
    # - é˜¶å±‚å€æ•°ï¼šQ1-Q5å„é˜¶å±‚ç›¸å¯¹äºåŸºç¡€æˆæœ¬çš„å€æ•°
    # ========================================
    business_base_cost = 30   # [CONFIG] åŸºç¡€æˆæœ¬ï¼ˆå…ƒ/å‘˜å·¥/æœˆï¼‰
    business_stratum_multipliers = [1.0, 1.7, 2.8, 4.2, 7.5]  # [CONFIG] Q1-Q5é˜¶å±‚å€æ•°
    
    # é¢„è®¾æ–¹æ¡ˆï¼ˆå¯é€‰ï¼‰
    BUSINESS_COST_PRESETS = {
        'default': {  # é»˜è®¤ï¼šåŸºäºLorenzæ›²çº¿
            'base_cost': 200,
            'multipliers': [1.0, 2.0, 3.25, 5.0, 13.75]
        },
        'moderate': {  # é€‚åº¦å·®å¼‚ï¼šQ5æ˜¯Q1çš„6å€
            'base_cost': 200, 
        },
        'uniform': {  # ç»Ÿä¸€æˆæœ¬ï¼šæ‰€æœ‰é˜¶å±‚ç›¸åŒ
            'base_cost': 200,
            'multipliers': [1.0, 1.0, 1.0, 1.0, 1.0]
        },
        'extreme': {  # æç«¯å·®å¼‚ï¼šQ5æ˜¯Q1çš„20å€
            'base_cost': 200,
            'multipliers': [1.0, 2.5, 5.0, 10.0, 20.0]
        }
    }
    
    # [CONFIG] å¿«é€Ÿåˆ‡æ¢é¢„è®¾ï¼ˆæ³¨é‡Šæ‰ä¸‹é¢ä¸¤è¡Œå¯ä½¿ç”¨ä¸Šé¢çš„æ‰‹åŠ¨é…ç½®ï¼‰
    # preset = BUSINESS_COST_PRESETS['default']
    # business_base_cost, business_stratum_multipliers = preset['base_cost'], preset['multipliers']
    
    print(f"ğŸ’° Economic Configuration:")
    print(f"  - Total Wealth: {total_wealth:,}")
    print(f"  - Business Share: {business_gdp_share*100}% = {total_wealth*business_gdp_share:,.0f}")
    print(f"  - Government Share: {public_gdp_share*100}% = {total_wealth*public_gdp_share:,.0f}")
    print(f"  - Person Share: {(1-business_gdp_share-public_gdp_share)*100}% = {total_wealth*(1-business_gdp_share-public_gdp_share):,.0f}")
    print(f"  - Economy Openness: {economy_openness} ({'Closed' if economy_openness == 0.0 else 'Open' if economy_openness == 1.0 else 'Moderate'})")
    print(f"    * Capital outflow: House {economy_openness*90:.1f}%, Business {economy_openness*67:.1f}%")
    print(f"    * Local circulation: House {(1-economy_openness)*90:.1f}%, Business {(1-economy_openness)*67:.1f}%")
    print(f"\nğŸ’¼ Business Operating Cost Configuration:")
    print(f"  - Base Cost: ${business_base_cost}/employee/month")
    print(f"  - Stratum Multipliers: {business_stratum_multipliers}")
    print(f"  - Q1 (street shop): ${business_base_cost * business_stratum_multipliers[0]:.0f}/employee/month")
    print(f"  - Q3 (chain store): ${business_base_cost * business_stratum_multipliers[2]:.0f}/employee/month")
    print(f"  - Q5 (luxury store): ${business_base_cost * business_stratum_multipliers[4]:.0f}/employee/month")
    print(f"  - Cost Ratio (Q5/Q1): {business_stratum_multipliers[4]/business_stratum_multipliers[0]:.2f}Ã—")
    print()
    
    # Initialize economic logger if enabled
    if ENABLE_ECONOMIC_DEBUG:
        print("\n" + "="*80)
        print("  ğŸ’° ECONOMIC DEBUG LOGGING ENABLED")
        print("="*80)
        economic_logger.initialize(enabled=True, output_dir=output_dir)
        print(f"  Economic transactions will be logged for detailed analysis")
        print(f"  This will help identify wealth conservation issues")
        print()
    else:
        economic_logger.initialize(enabled=False)

    # Clear debug log file at the start of each experiment
    if ENABLE_ECONOMIC_DEBUG:
        from covid_abs.network.log_config import clear_debug_log
        clear_debug_log()
        print(f"ğŸ“ Debug log cleared: debug_cashflow.log")
    
    # Note: Detailed logs are written to debug_cashflow.log
    # Use sort_debug_logs.py to sort them after experiment completion
    print(f"ğŸ“ Detailed logs: debug_cashflow.log (use sort_debug_logs.py to sort)")
    
    # Run batch experiment
    df = batch_experiment(
        experiments=experiments,
        iterations=iterations,
        file=csv_file,
        simulation_type=GraphSimulation,
        verbose='experiments',
        
        # ========================================
        # ç¯å¢ƒå‚æ•°
        # ========================================
        population_size=population_size,
        length=207,                         # âœ… ä¿æŒ24äºº/kmÂ²å¯†åº¦
        height=207,                         # âœ… ä¿æŒ24äºº/kmÂ²å¯†åº¦
        
        # ========================================
        # ç¤¾ä¼šäººå£å‚æ•°ï¼ˆğŸ“š çœŸå®æ•°æ®ï¼‰
        # ========================================
        homemates_avg=3,                    # âœ… [44] å®¶åº­è§„æ¨¡
        homemates_std=1,                    # âœ… å®¶åº­è§„æ¨¡æ ‡å‡†å·®
        homeless_rate=0.0005,               # âœ… [45] 0.05%æ— å®¶å¯å½’ç‡
        unemployment_rate=0.12,             # âœ… [54] 12%å¤±ä¸šç‡
        
        # ========================================
        # æµè¡Œç—…å­¦å‚æ•°ï¼ˆğŸ“š çœŸå®æ•°æ®ï¼‰
        # ========================================
        # åˆå§‹çŠ¶æ€
        initial_infected_perc=0.01,         # âœ… 1%åˆå§‹æ„ŸæŸ“
        initial_immune_perc=0.01,           # âœ… 1%åˆå§‹å…ç–«
        
        # ä¼ æŸ“å‚æ•°
        contagion_distance=1.0,             # âœ… [46] ç¤¾äº¤è·ç¦»
        contagion_rate=0.9,                 # âœ… [46] 90%ä¼ æŸ“ç‡
        
        # ç–¾ç—…æ—¶é—´å‚æ•°ï¼ˆå¤©ï¼‰
        incubation_time=5,                  # âœ… [47,48] 5å¤©æ½œä¼æœŸ
        contagion_time=10,                  # âœ… [49] 10å¤©ä¼ æŸ“æœŸ
        recovering_time=20,                 # âœ… [50] 20å¤©åº·å¤æœŸ
        
        # åŒ»ç–—ç³»ç»Ÿ
        critical_limit=0.05,                # âœ… 5% ICUå®¹é‡
        hospitalization_capacity=0.05,      # âœ… 5%ä½é™¢å®¹é‡
        
        # ç§»åŠ¨å‚æ•°
        amplitudes={                        # âœ… [Table 2] Î±6=10å•ä½ç§»åŠ¨å¹…åº¦
            Status.Susceptible: 10,
            Status.Recovered_Immune: 10,
            Status.Infected: 10
        },
        
        # ä¼ä¸šå‚æ•°
        total_business=5,                  # âœ… [53] 15å®¶ä¼ä¸š
        business_distance=20,               # âœ… ä¼ä¸šè·ç¦»
        
        # ========================================
        # ç»æµå‚æ•°
        # ========================================
        total_wealth=total_wealth,
        business_gdp_share=business_gdp_share,
        public_gdp_share=public_gdp_share,
        minimum_income=minimum_income,
        minimum_expense=minimum_expense,
        economy_openness=economy_openness,
        business_base_cost=business_base_cost,
        business_stratum_multipliers=business_stratum_multipliers,
        
        # ========================================
        # LLM & Scenarioé…ç½®
        # ========================================
        scenario_config=SCENARIO_CONFIG,
        backend=backend,
        enable_llm_decision=True,
        max_concurrent_llm=max_concurrent_llm,
        llm_log_file=llm_log_file
    )
    
    print("\n" + "="*80)
    print("  Experiment Complete!")
    print("="*80)
    print(f"\nâœ“ Standard CSV output: {csv_file}")
    print(f"  Format: [Iteration, Metric, Min, Avg, Std, Max]")
    print(f"  Compatible with original ABS analysis tools")
    
    if ENABLE_LLM_LOGGING:
        print(f"\nâœ“ LLM decision logs: {llm_log_file}")
        print(f"  Contains all agent decisions with:")
        print(f"  - iteration, day, hour")
        print(f"  - agent_id, agent_type (Person/Business/Government)")
        print(f"  - action, reasoning, parameters")
        print(f"  - experiment number")
    else:
        print(f"\n  (LLM logging disabled - set ENABLE_LLM_LOGGING=True to enable)")
    
    # ğŸ”§ [ENHANCED] æ˜¾ç¤ºAPIå¯†é’¥ä½¿ç”¨ç»Ÿè®¡
    if backend and hasattr(backend, 'print_key_stats'):
        backend.print_key_stats()
    
    print("\n" + "="*80)
    
    # Close console logging
    try:
        sys.stdout.close()
        sys.stdout = tee.terminal  # Restore original stdout
        print(f"ğŸ“ Console output saved to: {console_log_file}")
    except:
        pass
    
    # Save economic debug log if enabled
    if ENABLE_ECONOMIC_DEBUG:
        print("\nğŸ’¾ Saving Economic Debug Log...")
        economic_logger.save()
        print("âœ“ Economic debug log saved successfully!")
        print("  Use the analysis script to diagnose wealth conservation issues")
        print()
    
    # Provide log sorting instructions
    print("\nğŸ“‹ Debug Log Processing:")
    print("  1. Sort detailed logs: python sort_debug_logs.py")
    print("  2. Analyze economics: python analyze_economic_debug.py")
    print("  3. View sorted logs: debug_cashflow_sorted.log")
    
    # Display sample results
    if df is not None:
        print("\nğŸ“Š Sample Results (first 15 rows):")
        print(df.head(15).to_string())
        print(f"\n   ... ({len(df)} total rows)\n")
    
    # Summary of generated files
    print("\n" + "="*80)
    print("  ğŸ“ GENERATED FILES SUMMARY")
    print("="*80)
    print(f"\nâœ… Essential outputs:")
    print(f"  â€¢ Results CSV: {csv_file}")
    
    if ENABLE_LLM_LOGGING and llm_log_file:
        print(f"  â€¢ LLM Decision Log: {llm_log_file}")
    
    if ENABLE_CONSOLE_OUTPUT and tee:
        print(f"\nâœ… Debug outputs:")
        print(f"  â€¢ Console Output: Saved to log file")
    
    if ENABLE_ECONOMIC_DEBUG:
        economic_files = [f for f in os.listdir(output_dir) if f.startswith('economic_debug_')]
        if economic_files:
            print(f"  â€¢ Economic Debug: {len(economic_files)} JSON files")
    
    if ENABLE_CASHFLOW_DEBUG:
        cashflow_file = os.path.join(output_dir, "debug_cashflow.log")
        if os.path.exists(cashflow_file):
            print(f"  â€¢ Cashflow Debug: debug_cashflow.log")
    
    # Summary of disabled outputs
    disabled = []
    if not ENABLE_LLM_LOGGING:
        disabled.append("LLM logs")
    if not ENABLE_CONSOLE_OUTPUT:
        disabled.append("Console output")
    if not ENABLE_ECONOMIC_DEBUG:
        disabled.append("Economic debug")
    if not ENABLE_CASHFLOW_DEBUG:
        disabled.append("Cashflow debug")
    if not ENABLE_HIRING_DEBUG:
        disabled.append("Hiring prints")
    if not ENABLE_GOVERNMENT_DEBUG:
        disabled.append("Government prints")
    
    if disabled:
        print(f"\nâš ï¸ Disabled outputs: {', '.join(disabled)}")
        print(f"   (To enable, modify flags at top of script)")
    
    print("\nâœ“ Done! You can now:")
    print("  1. Visualize results: python visualize_graph_batch.py")
    if ENABLE_LLM_LOGGING:
        print(f"  2. Analyze LLM decisions: python analyze_llm_logs.py --log_file {llm_log_file}")
    print("  3. Compare with other experiments")
    print("  4. Adjust configuration flags for next run")
    print()


if __name__ == "__main__":
    # Try to import lock mechanism (optional)
    try:
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'tools', 'experiment_management'))
        from experiment_lock import experiment_lock
        
        # Acquire lock before starting experiment
        if not experiment_lock.acquire():
            print("âŒ Failed to acquire experiment lock. Exiting...")
            sys.exit(1)
        
        lock_acquired = True
    except ImportError:
        # Lock mechanism not available, proceed without it
        print("âš ï¸ Experiment lock not available, proceeding without lock protection")
        print("   (To enable, ensure experiment_lock.py is in tools/experiment_management/)")
        lock_acquired = False
        experiment_lock = None
    
    try:
        main()
    finally:
        # Ensure lock is released if it was acquired
        if lock_acquired and experiment_lock:
            experiment_lock.release()
